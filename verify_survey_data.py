# -----------------------------------------------------------------------------------
# AG2 Report Verification Script
#
# Purpose:
#   This script verifies that the two survey reports generated by process_survey_data_1
#   and process_survey_data_2 contain identical statistical results, regardless of
#   differences in formatting, structure, or wording.
#
# How it works:
#   - Reads both reports into shared context variables.
#   - Uses an AI verification agent to compare all reported statistics.
#   - Returns TRUE if all counts, percentages, and other numeric results match exactly.
#   - Returns FALSE and detailed discrepancy notes if mismatches are found.
#
# Dependencies:
#   pip install python-dotenv ag2 pydantic
#
# Requirements:
#   - Must run after both survey reports are generated.
#   - OPENAI_API_KEY must be available in your environment (.env file).
# -----------------------------------------------------------------------------------

from dotenv import load_dotenv
import os
from autogen.agentchat.group import AgentTarget, ContextVariables, ReplyResult, TerminateTarget
from autogen import UserProxyAgent, ConversableAgent, LLMConfig, UpdateSystemMessage
from autogen.agentchat.group.patterns import DefaultPattern
from autogen.agentchat import initiate_group_chat
from pydantic import BaseModel, Field

# Load environment variables from .env file
load_dotenv()

def run_verification(model: str):
    """
    Executes the verification workflow to ensure that two survey reports
    contain identical statistical results.

    Parameters
    ----------
    model : str
        The OpenAI model name to be used for AI-based verification (e.g., "gpt-4").

    Workflow
    --------
    1. Initialize shared context variables for storing report content and verification state.
    2. Define functions for:
       - Reading survey reports from disk.
       - Submitting verification status and feedback.
    3. Create agents:
       - read_reports_agent: Loads both reports into shared context.
       - verify_reports_agent: Compares reports and determines verification result.
       - user: Represents the requester (not interactive in this workflow).
    4. Orchestrate agent interactions using DefaultPattern.
    5. Output verification result and feedback to console.
    """

    print("Initiating the Report Verification Process...")

    # ---------------------------
    # LLM configuration
    # ---------------------------
    llm_config = LLMConfig(
        api_type="openai",
        model=model,
        api_key=os.getenv("OPENAI_API_KEY"),
        temperature=0,  # Deterministic output for reproducibility
        cache_seed=None,
    )

    # ---------------------------
    # Shared context variables
    # ---------------------------
    shared_context = ContextVariables(data={
        "survey_report_1": "",
        "survey_report_2": "",
        "verified": False,   # Final verification result (True/False)

        # Error handling fields
        "has_error": False,
        "error_message": "",
        "error_stage": ""
    })

    # ---------------------------
    # Tool: Read survey reports
    # ---------------------------
    def read_survey_reports(context_variables: ContextVariables) -> ReplyResult:
        """
        Reads both survey reports from disk and stores their content in shared context.

        Parameters
        ----------
        context_variables : ContextVariables
            Shared context object for passing data between agents.

        Returns
        -------
        ReplyResult
            Next target agent and updated context.

        Raises
        ------
        FileNotFoundError
            If either report file cannot be found.
        """
        file_paths = {
            "survey_report_1": "Report 1/survey_results_run_1.md",
            "survey_report_2": "Report 2/survey_results_run_2.md",
        }

        try:
            for key, path in file_paths.items():
                with open(path, "r", encoding="utf-8") as f:
                    context_variables[key] = f.read()
        except FileNotFoundError as e:
            missing = e.filename or "unknown"
            context_variables["has_error"] = True
            context_variables["error_message"] = f"Could not find survey report at '{missing}'"
            context_variables["error_stage"] = "read_survey_reports"
            raise

        return ReplyResult(
            message="Reports read successfully.",
            target=AgentTarget(verify_reports_agent),
            context_variables=context_variables,
        )

    # ---------------------------
    # Verification status schema
    # ---------------------------
    class VerificationStatus(BaseModel):
        verified: bool = Field(..., description="Verification status (True/False)")
        feedback: str = Field(..., description="Detailed feedback on verification result")

    # ---------------------------
    # Tool: Submit verification status
    # ---------------------------
    def submit_verification_status(verified: bool, feedback: str, context_variables: ContextVariables) -> ReplyResult:
        """
        Records the verification result in shared context and terminates the workflow.

        Parameters
        ----------
        verified : bool
            Whether the reports are consistent (True) or not (False).
        feedback : str
            Explanation for the verification result.
        context_variables : ContextVariables
            Shared context object for passing data between agents.

        Returns
        -------
        ReplyResult
            Terminates the agent workflow after storing results.
        """
        context_variables["verified"] = verified
        context_variables["feedback"] = feedback
        return ReplyResult(
            message=f"Verification status submitted successfully: verified={verified}",
            target=TerminateTarget(),
            context_variables=context_variables,
        )

    # ---------------------------
    # Agent definitions
    # ---------------------------
    with llm_config:
        # Agent that reads both reports
        read_reports_agent = ConversableAgent(
            name="read_reports_agent",
            system_message="""You are the agent that reads the survey reports.
            Your task is to read the survey reports and store them in the shared context.

            Use the read_survey_reports tool to read the survey reports.""",
            functions=[read_survey_reports]
        )

        # Agent that verifies statistical consistency
        verify_reports_agent = ConversableAgent(
            name="verify_reports_agent",
            system_message="""You are the agent responsible for verifying the consistency of two survey reports.""",
            functions=[submit_verification_status],
            update_agent_state_before_reply=[
                UpdateSystemMessage(
                    """
                    You are the agent responsible for verifying the consistency of two survey reports.
                    Your task is to examine 'survey_report_1' and 'survey_report_2' and verify that they are consistent.

                    Here is 'survey_report_1':
                    {survey_report_1}

                    Here is 'survey_report_2':
                    {survey_report_2}

                    These reports may differ in structure, wording, or formatting — that is acceptable. However, the **statistical results** 
                    (such as percentages, counts, means, etc.) must be the same in both reports.

                    Compare the statistics presented in each report. If all the statistics are consistent across both reports, 
                    verification shall be considered successful (TRUE). If there are any discrepancies in the statistics — regardless 
                    of formatting or phrasing — verification shall be considered unsuccessful (FALSE).

                    If verification is unsuccessful, provide a detailed explanation of the discrepancies in your feedback.

                    If verification is successful, explain that you have confirmed and verified the consistency of the reports in your feedback.

                    Use the submit_verification_status tool to submit your verification result and feedback.

                    After the verification has been submitted, reply with 'TERMINATE'.
                    """
                )
            ]
        )

    # ---------------------------
    # User agent
    # ---------------------------
    user = UserProxyAgent(
        name="user",
        code_execution_config=False
    )

    # Define workflow handoffs
    read_reports_agent.handoffs.set_after_work(AgentTarget(verify_reports_agent))
    verify_reports_agent.handoffs.set_after_work(TerminateTarget())

    # ---------------------------
    # Orchestration pattern
    # ---------------------------
    agent_pattern = DefaultPattern(
        initial_agent=read_reports_agent,
        agents=[
            read_reports_agent,
            verify_reports_agent,
        ],
        context_variables=shared_context,
        user_agent=user,
        group_manager_args={
            "llm_config": llm_config,
            # Stop when the agent returns exactly "TERMINATE"
            "is_termination_msg": lambda msg: msg.get("content", "").strip() == "TERMINATE"
        }
    )

    # ---------------------------
    # Run verification process
    # ---------------------------
    chat_result, final_context, last_agent = initiate_group_chat(
        pattern=agent_pattern,
        messages="Verify the consistency of the statistics in the two survey reports.",
        max_rounds=10,
    )

    # ---------------------------
    # Output verification result
    # ---------------------------
    if final_context.get("verified"):
        print("Report verification completed successfully!")
        print("Verification Feedback:", final_context.get("feedback"))
    else:
        print("Report verification did not complete successfully.")
        print("Verification Feedback:", final_context.get("feedback"))
